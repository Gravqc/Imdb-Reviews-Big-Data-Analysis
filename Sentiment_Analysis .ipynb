{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53673e78",
   "metadata": {},
   "source": [
    "# IMDB DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "384392d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: your-package-name in /Users/apple/.local/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from your-package-name) (1.22.3)\n",
      "Requirement already satisfied: pandas in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from your-package-name) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from pandas->your-package-name) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from pandas->your-package-name) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from pandas->your-package-name) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->your-package-name) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user your-package-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df847c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensor2tensor in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (1.15.7)\n",
      "Requirement already satisfied: absl-py in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (1.4.0)\n",
      "Requirement already satisfied: bz2file in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.98)\n",
      "Requirement already satisfied: dopamine-rl in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (3.2.1)\n",
      "Requirement already satisfied: flask in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (3.0.0)\n",
      "Requirement already satisfied: future in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.18.3)\n",
      "Requirement already satisfied: gevent in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (23.9.1)\n",
      "Requirement already satisfied: gin-config in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.5.0)\n",
      "Requirement already satisfied: google-api-python-client in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (2.107.0)\n",
      "Requirement already satisfied: gunicorn in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (21.2.0)\n",
      "Requirement already satisfied: gym in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.26.2)\n",
      "Requirement already satisfied: h5py in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (3.6.0)\n",
      "Requirement already satisfied: kfac in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.2.0)\n",
      "Requirement already satisfied: mesh-tensorflow in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.1.21)\n",
      "Requirement already satisfied: numpy in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (1.22.3)\n",
      "Requirement already satisfied: oauth2client in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (4.1.3)\n",
      "Requirement already satisfied: opencv-python in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (4.8.1.78)\n",
      "Requirement already satisfied: Pillow in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (10.0.1)\n",
      "Requirement already satisfied: pypng in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.20220715.0)\n",
      "Requirement already satisfied: requests in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (2.31.0)\n",
      "Requirement already satisfied: scipy in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (1.10.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (1.16.0)\n",
      "Requirement already satisfied: sympy in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (1.12)\n",
      "Requirement already satisfied: tensorflow-addons in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.21.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (4.9.2)\n",
      "Requirement already satisfied: tensorflow-gan in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-probability==0.7.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (0.7.0)\n",
      "Requirement already satisfied: tf-slim in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensor2tensor) (4.66.1)\n",
      "Requirement already satisfied: decorator in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-probability==0.7.0->tensor2tensor) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=0.6.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-probability==0.7.0->tensor2tensor) (3.0.0)\n",
      "Requirement already satisfied: tensorflow>=2.2.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from dopamine-rl->tensor2tensor) (2.13.0)\n",
      "Requirement already satisfied: flax>=0.2.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from dopamine-rl->tensor2tensor) (0.7.2)\n",
      "Requirement already satisfied: jax>=0.1.72 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from dopamine-rl->tensor2tensor) (0.4.13)\n",
      "Requirement already satisfied: jaxlib>=0.1.51 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from dopamine-rl->tensor2tensor) (0.4.13)\n",
      "Requirement already satisfied: pygame>=1.9.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from dopamine-rl->tensor2tensor) (2.5.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from dopamine-rl->tensor2tensor) (2.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flask->tensor2tensor) (3.0.0)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flask->tensor2tensor) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flask->tensor2tensor) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flask->tensor2tensor) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flask->tensor2tensor) (1.7.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flask->tensor2tensor) (6.8.0)\n",
      "Requirement already satisfied: zope.event in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from gevent->tensor2tensor) (5.0)\n",
      "Requirement already satisfied: zope.interface in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from gevent->tensor2tensor) (6.1)\n",
      "Requirement already satisfied: greenlet>=2.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from gevent->tensor2tensor) (3.0.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-api-python-client->tensor2tensor) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-api-python-client->tensor2tensor) (2.23.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-api-python-client->tensor2tensor) (0.1.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-api-python-client->tensor2tensor) (2.13.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-api-python-client->tensor2tensor) (4.1.1)\n",
      "Requirement already satisfied: packaging in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from gunicorn->tensor2tensor) (23.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from gym->tensor2tensor) (0.0.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from oauth2client->tensor2tensor) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from oauth2client->tensor2tensor) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from oauth2client->tensor2tensor) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->tensor2tensor) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->tensor2tensor) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->tensor2tensor) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->tensor2tensor) (2023.7.22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath>=0.19 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from sympy->tensor2tensor) (1.3.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-addons->tensor2tensor) (2.13.3)\n",
      "Requirement already satisfied: array-record in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (0.4.0)\n",
      "Requirement already satisfied: dm-tree in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (0.1.8)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (1.3.0)\n",
      "Requirement already satisfied: promise in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (3.20.3)\n",
      "Requirement already satisfied: psutil in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (5.9.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (2.3.0)\n",
      "Requirement already satisfied: toml in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (0.10.2)\n",
      "Requirement already satisfied: wrapt in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-datasets->tensor2tensor) (6.1.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-gan->tensor2tensor) (0.15.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tensor2tensor) (4.5.0)\n",
      "Requirement already satisfied: zipp in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tensor2tensor) (3.17.0)\n",
      "Requirement already satisfied: msgpack in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (1.0.7)\n",
      "Requirement already satisfied: optax in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (0.1.7)\n",
      "Requirement already satisfied: orbax-checkpoint in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (0.2.3)\n",
      "Requirement already satisfied: tensorstore in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (0.1.45)\n",
      "Requirement already satisfied: rich>=11.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (13.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor) (6.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client->tensor2tensor) (1.61.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client->tensor2tensor) (5.3.1)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from gym->tensor2tensor) (0.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client->tensor2tensor) (2.4.7)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from jax>=0.1.72->dopamine-rl->tensor2tensor) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from jax>=0.1.72->dopamine-rl->tensor2tensor) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from Jinja2>=3.1.2->flask->tensor2tensor) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor) (2023.3)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (2.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (16.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (68.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (1.48.2)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (2.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor) (2.16.1)\n",
      "Requirement already satisfied: chex>=0.1.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from optax->flax>=0.2.0->dopamine-rl->tensor2tensor) (0.1.7)\n",
      "Requirement already satisfied: cached_property in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl->tensor2tensor) (1.5.2)\n",
      "Requirement already satisfied: nest_asyncio in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl->tensor2tensor) (1.5.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.41.2)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from chex>=0.1.5->optax->flax>=0.2.0->dopamine-rl->tensor2tensor) (0.12.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mdurl~=0.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor) (0.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (0.7.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --default-timeout=100 tensor2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e0d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (2.13.0)\n",
      "Requirement already satisfied: matplotlib in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (3.7.3)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.22.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.48.2)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from matplotlib) (6.1.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.23.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --default-timeout=100 tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05b931fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Other setup\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "# Setting up some directories\n",
    "data_dir = os.path.expanduser(\"~/t2t/data\")\n",
    "tmp_dir = os.path.expanduser(\"~/t2t/tmp\")\n",
    "train_dir = os.path.expanduser(\"~/t2t/train\")\n",
    "checkpoint_dir = os.path.expanduser(\"~/t2t/checkpoints\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba673a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset', 'ai2_arc', 'ai2_arc_with_ir', 'amazon_us_reviews', 'anli', 'answer_equivalence', 'arc', 'asqa', 'asset', 'assin2', 'bair_robot_pushing_small', 'bccd', 'beans', 'bee_dataset', 'beir', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'ble_wind_field', 'blimp', 'booksum', 'bool_q', 'bucc', 'c4', 'c4_wsrs', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cardiotox', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cfq', 'cherry_blossoms', 'chexpert', 'cifar10', 'cifar100', 'cifar100_n', 'cifar10_1', 'cifar10_corrupted', 'cifar10_n', 'citrus_leaves', 'cityscapes', 'civil_comments', 'clevr', 'clic', 'clinc_oos', 'cmaterdb', 'cnn_dailymail', 'coco', 'coco_captions', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'common_voice', 'conll2002', 'conll2003', 'controlled_noisy_web_labels', 'coqa', 'cos_e', 'cosmos_qa', 'covid19', 'covid19sum', 'crema_d', 'criteo', 'cs_restaurants', 'curated_breast_imaging_ddsm', 'cycle_gan', 'd4rl_adroit_door', 'd4rl_adroit_hammer', 'd4rl_adroit_pen', 'd4rl_adroit_relocate', 'd4rl_antmaze', 'd4rl_mujoco_ant', 'd4rl_mujoco_halfcheetah', 'd4rl_mujoco_hopper', 'd4rl_mujoco_walker2d', 'dart', 'davis', 'deep1b', 'deep_weeds', 'definite_pronoun_resolution', 'dementiabank', 'diabetic_retinopathy_detection', 'diamonds', 'div2k', 'dmlab', 'doc_nli', 'dolphin_number_word', 'domainnet', 'downsampled_imagenet', 'drop', 'dsprites', 'dtd', 'duke_ultrasound', 'e2e_cleaned', 'efron_morris75', 'emnist', 'eraser_multi_rc', 'esnli', 'eurosat', 'fashion_mnist', 'flic', 'flores', 'food101', 'forest_fires', 'fuss', 'gap', 'geirhos_conflict_stimuli', 'gem', 'genomics_ood', 'german_credit_numeric', 'gigaword', 'glove100_angular', 'glue', 'goemotions', 'gov_report', 'gpt3', 'gref', 'groove', 'grounded_scan', 'gsm8k', 'gtzan', 'gtzan_music_speech', 'hellaswag', 'higgs', 'hillstrom', 'horses_or_humans', 'howell', 'i_naturalist2017', 'i_naturalist2018', 'i_naturalist2021', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet2012_fewshot', 'imagenet2012_multilabel', 'imagenet2012_real', 'imagenet2012_subset', 'imagenet_a', 'imagenet_lt', 'imagenet_pi', 'imagenet_r', 'imagenet_resized', 'imagenet_sketch', 'imagenet_v2', 'imagenette', 'imagewang', 'imdb_reviews', 'irc_disentanglement', 'iris', 'istella', 'kddcup99', 'kitti', 'kmnist', 'laion400m', 'lambada', 'lfw', 'librispeech', 'librispeech_lm', 'libritts', 'ljspeech', 'lm1b', 'locomotion', 'lost_and_found', 'lsun', 'lvis', 'malaria', 'math_dataset', 'math_qa', 'mctaco', 'media_sum', 'mlqa', 'mnist', 'mnist_corrupted', 'movie_lens', 'movie_rationales', 'movielens', 'moving_mnist', 'mrqa', 'mslr_web', 'mt_opt', 'mtnt', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'natural_instructions', 'natural_questions', 'natural_questions_open', 'newsroom', 'nsynth', 'nyu_depth_v2', 'ogbg_molpcba', 'omniglot', 'open_images_challenge2019_detection', 'open_images_v4', 'openbookqa', 'opinion_abstracts', 'opinosis', 'opus', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'pass', 'patch_camelyon', 'paws_wiki', 'paws_x_wiki', 'penguins', 'pet_finder', 'pg19', 'piqa', 'places365_small', 'placesfull', 'plant_leaves', 'plant_village', 'plantae_k', 'protein_net', 'q_re_cc', 'qa4mre', 'qasc', 'quac', 'quality', 'quickdraw_bitmap', 'race', 'radon', 'reddit', 'reddit_disentanglement', 'reddit_tifu', 'ref_coco', 'resisc45', 'rlu_atari', 'rlu_atari_checkpoints', 'rlu_atari_checkpoints_ordered', 'rlu_control_suite', 'rlu_dmlab_explore_object_rewards_few', 'rlu_dmlab_explore_object_rewards_many', 'rlu_dmlab_rooms_select_nonmatching_object', 'rlu_dmlab_rooms_watermaze', 'rlu_dmlab_seekavoid_arena01', 'rlu_locomotion', 'rlu_rwrl', 'robomimic_mg', 'robomimic_mh', 'robomimic_ph', 'robonet', 'robosuite_panda_pick_place_can', 'rock_paper_scissors', 'rock_you', 's3o4d', 'salient_span_wikipedia', 'samsum', 'savee', 'scan', 'scene_parse150', 'schema_guided_dialogue', 'sci_tail', 'scicite', 'scientific_papers', 'scrolls', 'sentiment140', 'shapes3d', 'sift1m', 'simpte', 'siscore', 'smallnorb', 'smartwatch_gestures', 'snli', 'so2sat', 'speech_commands', 'spoken_digit', 'squad', 'squad_question_generation', 'stanford_dogs', 'stanford_online_products', 'star_cfq', 'starcraft_video', 'stl10', 'story_cloze', 'summscreen', 'sun397', 'super_glue', 'svhn_cropped', 'symmetric_solids', 'tao', 'tatoeba', 'ted_hrlr_translate', 'ted_multi_translate', 'tedlium', 'tf_flowers', 'the300w_lp', 'tiny_shakespeare', 'titanic', 'trec', 'trivia_qa', 'tydi_qa', 'uc_merced', 'ucf101', 'unified_qa', 'universal_dependencies', 'unnatural_instructions', 'user_libri_audio', 'user_libri_text', 'vctk', 'visual_domain_decathlon', 'voc', 'voxceleb', 'voxforge', 'waymo_open_dataset', 'web_graph', 'web_nlg', 'web_questions', 'webvid', 'wider_face', 'wiki40b', 'wiki_auto', 'wiki_bio', 'wiki_dialog', 'wiki_table_questions', 'wiki_table_text', 'wikiann', 'wikihow', 'wikipedia', 'wikipedia_toxicity_subtypes', 'wine_quality', 'winogrande', 'wit', 'wit_kaggle', 'wmt13_translate', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'wordnet', 'wsc273', 'xnli', 'xquad', 'xsum', 'xtreme_pawsx', 'xtreme_pos', 'xtreme_s', 'xtreme_xnli', 'yahoo_ltrc', 'yelp_polarity_reviews', 'yes_no', 'youtube_vis', 'huggingface:acronym_identification', 'huggingface:ade_corpus_v2', 'huggingface:adv_glue', 'huggingface:adversarial_qa', 'huggingface:aeslc', 'huggingface:afrikaans_ner_corpus', 'huggingface:ag_news', 'huggingface:ai2_arc', 'huggingface:air_dialogue', 'huggingface:ajgt_twitter_ar', 'huggingface:allegro_reviews', 'huggingface:allocine', 'huggingface:alt', 'huggingface:amazon_polarity', 'huggingface:amazon_reviews_multi', 'huggingface:amazon_us_reviews', 'huggingface:ambig_qa', 'huggingface:americas_nli', 'huggingface:ami', 'huggingface:amttl', 'huggingface:anli', 'huggingface:app_reviews', 'huggingface:aqua_rat', 'huggingface:aquamuse', 'huggingface:ar_cov19', 'huggingface:ar_res_reviews', 'huggingface:ar_sarcasm', 'huggingface:arabic_billion_words', 'huggingface:arabic_pos_dialect', 'huggingface:arabic_speech_corpus', 'huggingface:arcd', 'huggingface:arsentd_lev', 'huggingface:art', 'huggingface:arxiv_dataset', 'huggingface:ascent_kb', 'huggingface:aslg_pc12', 'huggingface:asnq', 'huggingface:asset', 'huggingface:assin', 'huggingface:assin2', 'huggingface:atomic', 'huggingface:autshumato', 'huggingface:babi_qa', 'huggingface:banking77', 'huggingface:bbaw_egyptian', 'huggingface:bbc_hindi_nli', 'huggingface:bc2gm_corpus', 'huggingface:beans', 'huggingface:best2009', 'huggingface:bianet', 'huggingface:bible_para', 'huggingface:big_patent', 'huggingface:bigbench', 'huggingface:billsum', 'huggingface:bing_coronavirus_query_set', 'huggingface:biomrc', 'huggingface:biosses', 'huggingface:biwi_kinect_head_pose', 'huggingface:blbooks', 'huggingface:blbooksgenre', 'huggingface:blended_skill_talk', 'huggingface:blimp', 'huggingface:blog_authorship_corpus', 'huggingface:bn_hate_speech', 'huggingface:bnl_newspapers', 'huggingface:bookcorpus', 'huggingface:bookcorpusopen', 'huggingface:boolq', 'huggingface:bprec', 'huggingface:break_data', 'huggingface:brwac', 'huggingface:bsd_ja_en', 'huggingface:bswac', 'huggingface:c3', 'huggingface:c4', 'huggingface:cail2018', 'huggingface:caner', 'huggingface:capes', 'huggingface:casino', 'huggingface:catalonia_independence', 'huggingface:cats_vs_dogs', 'huggingface:cawac', 'huggingface:cbt', 'huggingface:cc100', 'huggingface:cc_news', 'huggingface:ccaligned_multilingual', 'huggingface:cdsc', 'huggingface:cdt', 'huggingface:cedr', 'huggingface:cfq', 'huggingface:chr_en', 'huggingface:cifar10', 'huggingface:cifar100', 'huggingface:circa', 'huggingface:civil_comments', 'huggingface:clickbait_news_bg', 'huggingface:climate_fever', 'huggingface:clinc_oos', 'huggingface:clue', 'huggingface:cmrc2018', 'huggingface:cmu_hinglish_dog', 'huggingface:cnn_dailymail', 'huggingface:coached_conv_pref', 'huggingface:coarse_discourse', 'huggingface:codah', 'huggingface:code_search_net', 'huggingface:code_x_glue_cc_clone_detection_big_clone_bench', 'huggingface:code_x_glue_cc_clone_detection_poj104', 'huggingface:code_x_glue_cc_cloze_testing_all', 'huggingface:code_x_glue_cc_cloze_testing_maxmin', 'huggingface:code_x_glue_cc_code_completion_line', 'huggingface:code_x_glue_cc_code_completion_token', 'huggingface:code_x_glue_cc_code_refinement', 'huggingface:code_x_glue_cc_code_to_code_trans', 'huggingface:code_x_glue_cc_defect_detection', 'huggingface:code_x_glue_ct_code_to_text', 'huggingface:code_x_glue_tc_nl_code_search_adv', 'huggingface:code_x_glue_tc_text_to_code', 'huggingface:code_x_glue_tt_text_to_text', 'huggingface:com_qa', 'huggingface:common_gen', 'huggingface:common_language', 'huggingface:common_voice', 'huggingface:commonsense_qa', 'huggingface:competition_math', 'huggingface:compguesswhat', 'huggingface:conceptnet5', 'huggingface:conceptual_12m', 'huggingface:conceptual_captions', 'huggingface:conll2000', 'huggingface:conll2002', 'huggingface:conll2003', 'huggingface:conll2012_ontonotesv5', 'huggingface:conllpp', 'huggingface:consumer-finance-complaints', 'huggingface:conv_ai', 'huggingface:conv_ai_2', 'huggingface:conv_ai_3', 'huggingface:conv_questions', 'huggingface:coqa', 'huggingface:cord19', 'huggingface:cornell_movie_dialog', 'huggingface:cos_e', 'huggingface:cosmos_qa', 'huggingface:counter', 'huggingface:covid_qa_castorini', 'huggingface:covid_qa_deepset', 'huggingface:covid_qa_ucsd', 'huggingface:covid_tweets_japanese', 'huggingface:covost2', 'huggingface:cppe-5', 'huggingface:craigslist_bargains', 'huggingface:crawl_domain', 'huggingface:crd3', 'huggingface:crime_and_punish', 'huggingface:crows_pairs', 'huggingface:cryptonite', 'huggingface:cs_restaurants', 'huggingface:cuad', 'huggingface:curiosity_dialogs', 'huggingface:daily_dialog', 'huggingface:dane', 'huggingface:danish_political_comments', 'huggingface:dart', 'huggingface:datacommons_factcheck', 'huggingface:dbpedia_14', 'huggingface:dbrd', 'huggingface:deal_or_no_dialog', 'huggingface:definite_pronoun_resolution', 'huggingface:dengue_filipino', 'huggingface:dialog_re', 'huggingface:diplomacy_detection', 'huggingface:disaster_response_messages', 'huggingface:discofuse', 'huggingface:discovery', 'huggingface:disfl_qa', 'huggingface:doc2dial', 'huggingface:docred', 'huggingface:doqa', 'huggingface:dream', 'huggingface:drop', 'huggingface:duorc', 'huggingface:dutch_social', 'huggingface:dyk', 'huggingface:e2e_nlg', 'huggingface:e2e_nlg_cleaned', 'huggingface:ecb', 'huggingface:ecthr_cases', 'huggingface:eduge', 'huggingface:ehealth_kd', 'huggingface:eitb_parcc', 'huggingface:electricity_load_diagrams', 'huggingface:eli5', 'huggingface:eli5_category', 'huggingface:elkarhizketak', 'huggingface:emea', 'huggingface:emo', 'huggingface:emotion', 'huggingface:emotone_ar', 'huggingface:empathetic_dialogues', 'huggingface:enriched_web_nlg', 'huggingface:enwik8', 'huggingface:eraser_multi_rc', 'huggingface:esnli', 'huggingface:eth_py150_open', 'huggingface:ethos', 'huggingface:ett', 'huggingface:eu_regulatory_ir', 'huggingface:eurlex', 'huggingface:euronews', 'huggingface:europa_eac_tm', 'huggingface:europa_ecdc_tm', 'huggingface:europarl_bilingual', 'huggingface:event2Mind', 'huggingface:evidence_infer_treatment', 'huggingface:exams', 'huggingface:factckbr', 'huggingface:fake_news_english', 'huggingface:fake_news_filipino', 'huggingface:farsi_news', 'huggingface:fashion_mnist', 'huggingface:fever', 'huggingface:few_rel', 'huggingface:financial_phrasebank', 'huggingface:finer', 'huggingface:flores', 'huggingface:flue', 'huggingface:food101', 'huggingface:fquad', 'huggingface:freebase_qa', 'huggingface:gap', 'huggingface:gem', 'huggingface:generated_reviews_enth', 'huggingface:generics_kb', 'huggingface:german_legal_entity_recognition', 'huggingface:germaner', 'huggingface:germeval_14', 'huggingface:giga_fren', 'huggingface:gigaword', 'huggingface:glucose', 'huggingface:glue', 'huggingface:gnad10', 'huggingface:go_emotions', 'huggingface:gooaq', 'huggingface:google_wellformed_query', 'huggingface:grail_qa', 'huggingface:great_code', 'huggingface:greek_legal_code', 'huggingface:gsm8k', 'huggingface:guardian_authorship', 'huggingface:gutenberg_time', 'huggingface:hans', 'huggingface:hansards', 'huggingface:hard', 'huggingface:harem', 'huggingface:has_part', 'huggingface:hate_offensive', 'huggingface:hate_speech18', 'huggingface:hate_speech_filipino', 'huggingface:hate_speech_offensive', 'huggingface:hate_speech_pl', 'huggingface:hate_speech_portuguese', 'huggingface:hatexplain', 'huggingface:hausa_voa_ner', 'huggingface:hausa_voa_topics', 'huggingface:hda_nli_hindi', 'huggingface:head_qa', 'huggingface:health_fact', 'huggingface:hebrew_projectbenyehuda', 'huggingface:hebrew_sentiment', 'huggingface:hebrew_this_world', 'huggingface:hellaswag', 'huggingface:hendrycks_test', 'huggingface:hind_encorp', 'huggingface:hindi_discourse', 'huggingface:hippocorpus', 'huggingface:hkcancor', 'huggingface:hlgd', 'huggingface:hope_edi', 'huggingface:hotpot_qa', 'huggingface:hover', 'huggingface:hrenwac_para', 'huggingface:hrwac', 'huggingface:humicroedit', 'huggingface:hybrid_qa', 'huggingface:hyperpartisan_news_detection', 'huggingface:iapp_wiki_qa_squad', 'huggingface:id_clickbait', 'huggingface:id_liputan6', 'huggingface:id_nergrit_corpus', 'huggingface:id_newspapers_2018', 'huggingface:id_panl_bppt', 'huggingface:id_puisi', 'huggingface:igbo_english_machine_translation', 'huggingface:igbo_monolingual', 'huggingface:igbo_ner', 'huggingface:ilist', 'huggingface:imagenet-1k', 'huggingface:imagenet_sketch', 'huggingface:imdb', 'huggingface:imdb_urdu_reviews', 'huggingface:imppres', 'huggingface:indic_glue', 'huggingface:indonli', 'huggingface:indonlu', 'huggingface:inquisitive_qg', 'huggingface:interpress_news_category_tr', 'huggingface:interpress_news_category_tr_lite', 'huggingface:irc_disentangle', 'huggingface:isixhosa_ner_corpus', 'huggingface:isizulu_ner_corpus', 'huggingface:iwslt2017', 'huggingface:jeopardy', 'huggingface:jfleg', 'huggingface:jigsaw_toxicity_pred', 'huggingface:jigsaw_unintended_bias', 'huggingface:jnlpba', 'huggingface:journalists_questions', 'huggingface:kan_hope', 'huggingface:kannada_news', 'huggingface:kd_conv', 'huggingface:kde4', 'huggingface:kelm', 'huggingface:kilt_tasks', 'huggingface:kilt_wikipedia', 'huggingface:kinnews_kirnews', 'huggingface:klue', 'huggingface:kor_3i4k', 'huggingface:kor_hate', 'huggingface:kor_ner', 'huggingface:kor_nli', 'huggingface:kor_nlu', 'huggingface:kor_qpair', 'huggingface:kor_sae', 'huggingface:kor_sarcasm', 'huggingface:labr', 'huggingface:lama', 'huggingface:lambada', 'huggingface:large_spanish_corpus', 'huggingface:laroseda', 'huggingface:lc_quad', 'huggingface:lccc', 'huggingface:lener_br', 'huggingface:lex_glue', 'huggingface:liar', 'huggingface:librispeech_asr', 'huggingface:librispeech_lm', 'huggingface:limit', 'huggingface:lince', 'huggingface:linnaeus', 'huggingface:liveqa', 'huggingface:lj_speech', 'huggingface:lm1b', 'huggingface:lst20', 'huggingface:m_lama', 'huggingface:mac_morpho', 'huggingface:makhzan', 'huggingface:masakhaner', 'huggingface:math_dataset', 'huggingface:math_qa', 'huggingface:matinf', 'huggingface:mbpp', 'huggingface:mc4', 'huggingface:mc_taco', 'huggingface:md_gender_bias', 'huggingface:mdd', 'huggingface:med_hop', 'huggingface:medal', 'huggingface:medical_dialog', 'huggingface:medical_questions_pairs', 'huggingface:medmcqa', 'huggingface:menyo20k_mt', 'huggingface:meta_woz', 'huggingface:metashift', 'huggingface:metooma', 'huggingface:metrec', 'huggingface:miam', 'huggingface:mkb', 'huggingface:mkqa', 'huggingface:mlqa', 'huggingface:mlsum', 'huggingface:mnist', 'huggingface:mocha', 'huggingface:monash_tsf', 'huggingface:moroco', 'huggingface:movie_rationales', 'huggingface:mrqa', 'huggingface:ms_marco', 'huggingface:ms_terms', 'huggingface:msr_genomics_kbcomp', 'huggingface:msr_sqa', 'huggingface:msr_text_compression', 'huggingface:msr_zhen_translation_parity', 'huggingface:msra_ner', 'huggingface:mt_eng_vietnamese', 'huggingface:muchocine', 'huggingface:multi_booked', 'huggingface:multi_eurlex', 'huggingface:multi_news', 'huggingface:multi_nli', 'huggingface:multi_nli_mismatch', 'huggingface:multi_para_crawl', 'huggingface:multi_re_qa', 'huggingface:multi_woz_v22', 'huggingface:multi_x_science_sum', 'huggingface:multidoc2dial', 'huggingface:multilingual_librispeech', 'huggingface:mutual_friends', 'huggingface:mwsc', 'huggingface:myanmar_news', 'huggingface:narrativeqa', 'huggingface:narrativeqa_manual', 'huggingface:natural_questions', 'huggingface:ncbi_disease', 'huggingface:nchlt', 'huggingface:ncslgr', 'huggingface:nell', 'huggingface:neural_code_search', 'huggingface:news_commentary', 'huggingface:newsgroup', 'huggingface:newsph', 'huggingface:newsph_nli', 'huggingface:newspop', 'huggingface:newsqa', 'huggingface:newsroom', 'huggingface:nkjp-ner', 'huggingface:nli_tr', 'huggingface:nlu_evaluation_data', 'huggingface:norec', 'huggingface:norne', 'huggingface:norwegian_ner', 'huggingface:nq_open', 'huggingface:nsmc', 'huggingface:numer_sense', 'huggingface:numeric_fused_head', 'huggingface:oclar', 'huggingface:offcombr', 'huggingface:offenseval2020_tr', 'huggingface:offenseval_dravidian', 'huggingface:ofis_publik', 'huggingface:ohsumed', 'huggingface:ollie', 'huggingface:omp', 'huggingface:onestop_english', 'huggingface:onestop_qa', 'huggingface:open_subtitles', 'huggingface:openai_humaneval', 'huggingface:openbookqa', 'huggingface:openslr', 'huggingface:openwebtext', 'huggingface:opinosis', 'huggingface:opus100', 'huggingface:opus_books', 'huggingface:opus_dgt', 'huggingface:opus_dogc', 'huggingface:opus_elhuyar', 'huggingface:opus_euconst', 'huggingface:opus_finlex', 'huggingface:opus_fiskmo', 'huggingface:opus_gnome', 'huggingface:opus_infopankki', 'huggingface:opus_memat', 'huggingface:opus_montenegrinsubs', 'huggingface:opus_openoffice', 'huggingface:opus_paracrawl', 'huggingface:opus_rf', 'huggingface:opus_tedtalks', 'huggingface:opus_ubuntu', 'huggingface:opus_wikipedia', 'huggingface:opus_xhosanavy', 'huggingface:orange_sum', 'huggingface:oscar', 'huggingface:para_crawl', 'huggingface:para_pat', 'huggingface:parsinlu_reading_comprehension', 'huggingface:pass', 'huggingface:paws', 'huggingface:paws-x', 'huggingface:pec', 'huggingface:peer_read', 'huggingface:peoples_daily_ner', 'huggingface:per_sent', 'huggingface:persian_ner', 'huggingface:pg19', 'huggingface:php', 'huggingface:piaf', 'huggingface:pib', 'huggingface:piqa', 'huggingface:pn_summary', 'huggingface:poem_sentiment', 'huggingface:polemo2', 'huggingface:poleval2019_cyberbullying', 'huggingface:poleval2019_mt', 'huggingface:polsum', 'huggingface:polyglot_ner', 'huggingface:prachathai67k', 'huggingface:pragmeval', 'huggingface:proto_qa', 'huggingface:psc', 'huggingface:ptb_text_only', 'huggingface:pubmed', 'huggingface:pubmed_qa', 'huggingface:py_ast', 'huggingface:qa4mre', 'huggingface:qa_srl', 'huggingface:qa_zre', 'huggingface:qangaroo', 'huggingface:qanta', 'huggingface:qasc', 'huggingface:qasper', 'huggingface:qed', 'huggingface:qed_amara', 'huggingface:quac', 'huggingface:quail', 'huggingface:quarel', 'huggingface:quartz', 'huggingface:quickdraw', 'huggingface:quora', 'huggingface:quoref', 'huggingface:race', 'huggingface:re_dial', 'huggingface:reasoning_bg', 'huggingface:recipe_nlg', 'huggingface:reclor', 'huggingface:red_caps', 'huggingface:reddit', 'huggingface:reddit_tifu', 'huggingface:refresd', 'huggingface:reuters21578', 'huggingface:riddle_sense', 'huggingface:ro_sent', 'huggingface:ro_sts', 'huggingface:ro_sts_parallel', 'huggingface:roman_urdu', 'huggingface:roman_urdu_hate_speech', 'huggingface:ronec', 'huggingface:ropes', 'huggingface:rotten_tomatoes', 'huggingface:russian_super_glue', 'huggingface:rvl_cdip', 'huggingface:s2orc', 'huggingface:samsum', 'huggingface:sanskrit_classic', 'huggingface:saudinewsnet', 'huggingface:sberquad', 'huggingface:sbu_captions', 'huggingface:scan', 'huggingface:scb_mt_enth_2020', 'huggingface:scene_parse_150', 'huggingface:schema_guided_dstc8', 'huggingface:scicite', 'huggingface:scielo', 'huggingface:scientific_papers', 'huggingface:scifact', 'huggingface:sciq', 'huggingface:scitail', 'huggingface:scitldr', 'huggingface:search_qa', 'huggingface:sede', 'huggingface:selqa', 'huggingface:sem_eval_2010_task_8', 'huggingface:sem_eval_2014_task_1', 'huggingface:sem_eval_2018_task_1', 'huggingface:sem_eval_2020_task_11', 'huggingface:sent_comp', 'huggingface:senti_lex', 'huggingface:senti_ws', 'huggingface:sentiment140', 'huggingface:sepedi_ner', 'huggingface:sesotho_ner_corpus', 'huggingface:setimes', 'huggingface:setswana_ner_corpus', 'huggingface:sharc', 'huggingface:sharc_modified', 'huggingface:sick', 'huggingface:silicone', 'huggingface:simple_questions_v2', 'huggingface:siswati_ner_corpus', 'huggingface:smartdata', 'huggingface:sms_spam', 'huggingface:snips_built_in_intents', 'huggingface:snli', 'huggingface:snow_simplified_japanese_corpus', 'huggingface:so_stacksample', 'huggingface:social_bias_frames', 'huggingface:social_i_qa', 'huggingface:sofc_materials_articles', 'huggingface:sogou_news', 'huggingface:spanish_billion_words', 'huggingface:spc', 'huggingface:species_800', 'huggingface:speech_commands', 'huggingface:spider', 'huggingface:squad', 'huggingface:squad_adversarial', 'huggingface:squad_es', 'huggingface:squad_it', 'huggingface:squad_kor_v1', 'huggingface:squad_kor_v2', 'huggingface:squad_v1_pt', 'huggingface:squad_v2', 'huggingface:squadshifts', 'huggingface:srwac', 'huggingface:sst', 'huggingface:stereoset', 'huggingface:story_cloze', 'huggingface:stsb_mt_sv', 'huggingface:stsb_multi_mt', 'huggingface:style_change_detection', 'huggingface:subjqa', 'huggingface:super_glue', 'huggingface:superb', 'huggingface:svhn', 'huggingface:swag', 'huggingface:swahili', 'huggingface:swahili_news', 'huggingface:swda', 'huggingface:swedish_medical_ner', 'huggingface:swedish_ner_corpus', 'huggingface:swedish_reviews', 'huggingface:swiss_judgment_prediction', 'huggingface:tab_fact', 'huggingface:tamilmixsentiment', 'huggingface:tanzil', 'huggingface:tapaco', 'huggingface:tashkeela', 'huggingface:taskmaster1', 'huggingface:taskmaster2', 'huggingface:taskmaster3', 'huggingface:tatoeba', 'huggingface:ted_hrlr', 'huggingface:ted_iwlst2013', 'huggingface:ted_multi', 'huggingface:ted_talks_iwslt', 'huggingface:telugu_books', 'huggingface:telugu_news', 'huggingface:tep_en_fa_para', 'huggingface:text2log', 'huggingface:textvqa', 'huggingface:thai_toxicity_tweet', 'huggingface:thainer', 'huggingface:thaiqa_squad', 'huggingface:thaisum', 'huggingface:the_pile', 'huggingface:the_pile_books3', 'huggingface:the_pile_openwebtext2', 'huggingface:the_pile_stack_exchange', 'huggingface:tilde_model', 'huggingface:time_dial', 'huggingface:times_of_india_news_headlines', 'huggingface:timit_asr', 'huggingface:tiny_shakespeare', 'huggingface:tlc', 'huggingface:tmu_gfm_dataset', 'huggingface:tne', 'huggingface:told-br', 'huggingface:totto', 'huggingface:trec', 'huggingface:trivia_qa', 'huggingface:truthful_qa', 'huggingface:tsac', 'huggingface:ttc4900', 'huggingface:tunizi', 'huggingface:tuple_ie', 'huggingface:turk', 'huggingface:turkic_xwmt', 'huggingface:turkish_movie_sentiment', 'huggingface:turkish_ner', 'huggingface:turkish_product_reviews', 'huggingface:turkish_shrinked_ner', 'huggingface:turku_ner_corpus', 'huggingface:tweet_eval', 'huggingface:tweet_qa', 'huggingface:tweets_ar_en_parallel', 'huggingface:tweets_hate_speech_detection', 'huggingface:twi_text_c3', 'huggingface:twi_wordsim353', 'huggingface:tydiqa', 'huggingface:ubuntu_dialogs_corpus', 'huggingface:udhr', 'huggingface:um005', 'huggingface:un_ga', 'huggingface:un_multi', 'huggingface:un_pc', 'huggingface:universal_dependencies', 'huggingface:universal_morphologies', 'huggingface:urdu_fake_news', 'huggingface:urdu_sentiment_corpus', 'huggingface:vctk', 'huggingface:visual_genome', 'huggingface:vivos', 'huggingface:web_nlg', 'huggingface:web_of_science', 'huggingface:web_questions', 'huggingface:weibo_ner', 'huggingface:wi_locness', 'huggingface:wider_face', 'huggingface:wiki40b', 'huggingface:wiki_asp', 'huggingface:wiki_atomic_edits', 'huggingface:wiki_auto', 'huggingface:wiki_bio', 'huggingface:wiki_dpr', 'huggingface:wiki_hop', 'huggingface:wiki_lingua', 'huggingface:wiki_movies', 'huggingface:wiki_qa', 'huggingface:wiki_qa_ar', 'huggingface:wiki_snippets', 'huggingface:wiki_source', 'huggingface:wiki_split', 'huggingface:wiki_summary', 'huggingface:wikiann', 'huggingface:wikicorpus', 'huggingface:wikihow', 'huggingface:wikipedia', 'huggingface:wikisql', 'huggingface:wikitablequestions', 'huggingface:wikitext', 'huggingface:wikitext_tl39', 'huggingface:wili_2018', 'huggingface:wino_bias', 'huggingface:winograd_wsc', 'huggingface:winogrande', 'huggingface:wiqa', 'huggingface:wisesight1000', 'huggingface:wisesight_sentiment', 'huggingface:wmt14', 'huggingface:wmt15', 'huggingface:wmt16', 'huggingface:wmt17', 'huggingface:wmt18', 'huggingface:wmt19', 'huggingface:wmt20_mlqe_task1', 'huggingface:wmt20_mlqe_task2', 'huggingface:wmt20_mlqe_task3', 'huggingface:wmt_t2t', 'huggingface:wnut_17', 'huggingface:wongnai_reviews', 'huggingface:woz_dialogue', 'huggingface:wrbsc', 'huggingface:x_stance', 'huggingface:xcopa', 'huggingface:xcsr', 'huggingface:xed_en_fi', 'huggingface:xglue', 'huggingface:xnli', 'huggingface:xor_tydi_qa', 'huggingface:xquad', 'huggingface:xquad_r', 'huggingface:xsum', 'huggingface:xsum_factuality', 'huggingface:xtreme', 'huggingface:yahoo_answers_qa', 'huggingface:yahoo_answers_topics', 'huggingface:yelp_polarity', 'huggingface:yelp_review_full', 'huggingface:yoruba_bbc_topics', 'huggingface:yoruba_gv_ner', 'huggingface:yoruba_text_c3', 'huggingface:yoruba_wordsim353', 'huggingface:youtube_caption_corrections', 'huggingface:zest', 'kubric:kubric_frames', 'kubric:movi_a', 'kubric:movi_b', 'kubric:movi_c', 'kubric:movi_d', 'kubric:movi_e', 'kubric:movi_f', 'kubric:msn_easy', 'kubric:multi_shapenet_frames', 'kubric:nerf_synthetic_frames', 'kubric:nerf_synthetic_scenes', 'kubric:shapenet_pretraining', 'robotics:agent_aware_affordances', 'robotics:asu_table_top_converted_externally_to_rlds', 'robotics:austin_buds_dataset_converted_externally_to_rlds', 'robotics:austin_sailor_dataset_converted_externally_to_rlds', 'robotics:austin_sirius_dataset_converted_externally_to_rlds', 'robotics:bc_z', 'robotics:berkeley_autolab_ur5', 'robotics:berkeley_cable_routing', 'robotics:berkeley_fanuc_manipulation', 'robotics:berkeley_gnm_cory_hall', 'robotics:berkeley_gnm_recon', 'robotics:berkeley_gnm_sac_son', 'robotics:berkeley_mvp_converted_externally_to_rlds', 'robotics:berkeley_rpt_converted_externally_to_rlds', 'robotics:bridge', 'robotics:cmu_franka_exploration_dataset_converted_externally_to_rlds', 'robotics:cmu_play_fusion', 'robotics:cmu_playing_with_food', 'robotics:cmu_stretch', 'robotics:columbia_cairlab_pusht_real', 'robotics:dlr_edan_shared_control_converted_externally_to_rlds', 'robotics:dlr_sara_grid_clamp_converted_externally_to_rlds', 'robotics:dlr_sara_pour_converted_externally_to_rlds', 'robotics:eth_agent_affordances', 'robotics:fanuc_manipulation_v2', 'robotics:fractal20220817_data', 'robotics:furniture_bench_dataset_converted_externally_to_rlds', 'robotics:iamlab_cmu_pickup_insert_converted_externally_to_rlds', 'robotics:imperial_wrist_dataset', 'robotics:imperialcollege_sawyer_wrist_cam', 'robotics:jaco_play', 'robotics:kaist_nonprehensile_converted_externally_to_rlds', 'robotics:kuka', 'robotics:language_table', 'robotics:language_table_blocktoabsolute_oracle_sim', 'robotics:language_table_blocktoblock_4block_sim', 'robotics:language_table_blocktoblock_oracle_sim', 'robotics:language_table_blocktoblock_sim', 'robotics:language_table_blocktoblockrelative_oracle_sim', 'robotics:language_table_blocktorelative_oracle_sim', 'robotics:language_table_checkpoints', 'robotics:language_table_separate_oracle_sim', 'robotics:language_table_sim', 'robotics:maniskill_dataset_converted_externally_to_rlds', 'robotics:mt_opt_rlds', 'robotics:mt_opt_sd', 'robotics:mutex_dataset', 'robotics:nyu_door_opening_surprising_effectiveness', 'robotics:nyu_franka_play_dataset_converted_externally_to_rlds', 'robotics:nyu_rot_dataset_converted_externally_to_rlds', 'robotics:open_x_embodiment_and_rt_x_oss', 'robotics:qut_dexterous_manpulation', 'robotics:robo_net', 'robotics:robot_vqa', 'robotics:roboturk', 'robotics:stanford_hydra_dataset_converted_externally_to_rlds', 'robotics:stanford_kuka_multimodal_dataset_converted_externally_to_rlds', 'robotics:stanford_mask_vit_converted_externally_to_rlds', 'robotics:stanford_robocook_converted_externally_to_rlds', 'robotics:taco_play', 'robotics:tokyo_u_lsmo_converted_externally_to_rlds', 'robotics:toto', 'robotics:ucsd_kitchen_dataset_converted_externally_to_rlds', 'robotics:ucsd_pick_and_place_dataset_converted_externally_to_rlds', 'robotics:uiuc_d3field', 'robotics:usc_cloth_sim_converted_externally_to_rlds', 'robotics:utaustin_mutex', 'robotics:utokyo_pr2_opening_fridge_converted_externally_to_rlds', 'robotics:utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds', 'robotics:utokyo_saytap_converted_externally_to_rlds', 'robotics:utokyo_xarm_bimanual_converted_externally_to_rlds', 'robotics:utokyo_xarm_pick_and_place_converted_externally_to_rlds', 'robotics:viola']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# List of all datasets\n",
    "print(tfds.list_builders())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c560051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# IMDB dataset \n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439a09f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info: tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
      "    classification containing substantially more data than previous benchmark\n",
      "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
      "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='/Users/apple/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Info about the dataset\n",
    "print('Dataset Info:', info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ff9d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# Load the IMDB Reviews dataset\n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "# Example from the training dataset\n",
    "for example_text, example_label in train_dataset.take(1):\n",
    "    print(\"Text:\", example_text.numpy())\n",
    "    print(\"Label:\", example_label.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800adc72",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using PreTrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0961d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load the IMDB dataset\n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "# Prepare the data\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "# Extract sentences and labels from the training dataset\n",
    "for s, l in train_dataset:\n",
    "    train_sentences.append(str(s.numpy()))\n",
    "    train_labels.append(l.numpy())\n",
    "\n",
    "# Extract sentences and labels from the testing dataset\n",
    "for s, l in test_dataset:\n",
    "    test_sentences.append(str(s.numpy()))\n",
    "    test_labels.append(l.numpy())\n",
    "\n",
    "# Convert labels to numpy array\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Tokenize and pad the sentences\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=120, truncating='post', padding='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=120, truncating='post', padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c650fb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 16)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160289 (626.13 KB)\n",
      "Trainable params: 160289 (626.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=120),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a1e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.5264 - accuracy: 0.7623 - val_loss: 0.3925 - val_accuracy: 0.8270\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3136 - accuracy: 0.8695 - val_loss: 0.3739 - val_accuracy: 0.8341\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2572 - accuracy: 0.8982 - val_loss: 0.3851 - val_accuracy: 0.8314\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2221 - accuracy: 0.9146 - val_loss: 0.4127 - val_accuracy: 0.8257\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1962 - accuracy: 0.9267 - val_loss: 0.4470 - val_accuracy: 0.8188\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1757 - accuracy: 0.9357 - val_loss: 0.4956 - val_accuracy: 0.8101\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1589 - accuracy: 0.9433 - val_loss: 0.5193 - val_accuracy: 0.8127\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.1440 - accuracy: 0.9510 - val_loss: 0.5638 - val_accuracy: 0.8076\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1311 - accuracy: 0.9566 - val_loss: 0.6155 - val_accuracy: 0.8022\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.1195 - accuracy: 0.9611 - val_loss: 0.6656 - val_accuracy: 0.7976\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded, train_labels,\n",
    "                    epochs=10,\n",
    "                    validation_data=(test_padded, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1964fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6656 - accuracy: 0.7976\n",
      "Test accuracy: 0.7976400256156921\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_padded, test_labels)\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8f3e3",
   "metadata": {},
   "source": [
    "# BERT-BASE-UNCEASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf96183",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39a784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d570e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de920a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75665177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9fd039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bccde57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (0.16.0)\n",
      "Requirement already satisfied: filelock in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torchvision) (1.22.3)\n",
      "Requirement already satisfied: requests in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->torchvision) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->torchvision) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2c6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (0.16.0)\n",
      "Requirement already satisfied: torchaudio in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torchvision) (1.22.3)\n",
      "Requirement already satisfied: requests in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->torchvision) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->torchvision) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "868bebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization function\n",
    "def encode_reviews(data):\n",
    "    return tokenizer(data['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# Encode the input data\n",
    "train_dataset = train_dataset.map(encode_reviews, batched=True)\n",
    "test_dataset = test_dataset.map(encode_reviews, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88efab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "validation_dataloader = DataLoader(test_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79d9183b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (GPU) is available. Using GPU...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.  \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "# run this model on the CPU.\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS (GPU) is available. Using GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19556ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eaa7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "# Helper function to format elapsed times as hh:mm:ss\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "519a1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/mlp/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we could also have tried for more epochs.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45ffb850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (GPU) is available. Using GPU...\n",
      "Using device: mps\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,125.    Elapsed: 0:00:49.\n",
      "  Batch    80  of  3,125.    Elapsed: 0:01:36.\n",
      "  Batch   120  of  3,125.    Elapsed: 0:02:24.\n",
      "  Batch   160  of  3,125.    Elapsed: 0:03:11.\n",
      "  Batch   200  of  3,125.    Elapsed: 0:03:59.\n",
      "  Batch   240  of  3,125.    Elapsed: 0:04:46.\n",
      "  Batch   280  of  3,125.    Elapsed: 0:05:33.\n",
      "  Batch   320  of  3,125.    Elapsed: 0:06:20.\n",
      "  Batch   360  of  3,125.    Elapsed: 0:07:07.\n",
      "  Batch   400  of  3,125.    Elapsed: 0:07:54.\n",
      "  Batch   440  of  3,125.    Elapsed: 0:08:42.\n",
      "  Batch   480  of  3,125.    Elapsed: 0:09:29.\n",
      "  Batch   520  of  3,125.    Elapsed: 0:10:16.\n",
      "  Batch   560  of  3,125.    Elapsed: 0:11:03.\n",
      "  Batch   600  of  3,125.    Elapsed: 0:11:50.\n",
      "  Batch   640  of  3,125.    Elapsed: 0:12:37.\n",
      "  Batch   680  of  3,125.    Elapsed: 0:13:24.\n",
      "  Batch   720  of  3,125.    Elapsed: 0:14:12.\n",
      "  Batch   760  of  3,125.    Elapsed: 0:14:59.\n",
      "  Batch   800  of  3,125.    Elapsed: 0:15:46.\n",
      "  Batch   840  of  3,125.    Elapsed: 0:16:33.\n",
      "  Batch   880  of  3,125.    Elapsed: 0:17:20.\n",
      "  Batch   920  of  3,125.    Elapsed: 0:18:07.\n",
      "  Batch   960  of  3,125.    Elapsed: 0:18:55.\n",
      "  Batch 1,000  of  3,125.    Elapsed: 0:19:43.\n",
      "  Batch 1,040  of  3,125.    Elapsed: 0:20:31.\n",
      "  Batch 1,080  of  3,125.    Elapsed: 0:21:18.\n",
      "  Batch 1,120  of  3,125.    Elapsed: 0:22:06.\n",
      "  Batch 1,160  of  3,125.    Elapsed: 0:22:54.\n",
      "  Batch 1,200  of  3,125.    Elapsed: 0:23:42.\n",
      "  Batch 1,240  of  3,125.    Elapsed: 0:24:30.\n",
      "  Batch 1,280  of  3,125.    Elapsed: 0:25:18.\n",
      "  Batch 1,320  of  3,125.    Elapsed: 0:26:05.\n",
      "  Batch 1,360  of  3,125.    Elapsed: 0:26:53.\n",
      "  Batch 1,400  of  3,125.    Elapsed: 0:27:41.\n",
      "  Batch 1,440  of  3,125.    Elapsed: 0:28:28.\n",
      "  Batch 1,480  of  3,125.    Elapsed: 0:29:16.\n",
      "  Batch 1,520  of  3,125.    Elapsed: 0:30:04.\n",
      "  Batch 1,560  of  3,125.    Elapsed: 0:30:52.\n",
      "  Batch 1,600  of  3,125.    Elapsed: 0:31:40.\n",
      "  Batch 1,640  of  3,125.    Elapsed: 0:32:27.\n",
      "  Batch 1,680  of  3,125.    Elapsed: 0:33:15.\n",
      "  Batch 1,720  of  3,125.    Elapsed: 0:34:03.\n",
      "  Batch 1,760  of  3,125.    Elapsed: 0:34:51.\n",
      "  Batch 1,800  of  3,125.    Elapsed: 0:35:38.\n",
      "  Batch 1,840  of  3,125.    Elapsed: 0:36:26.\n",
      "  Batch 1,880  of  3,125.    Elapsed: 0:37:13.\n",
      "  Batch 1,920  of  3,125.    Elapsed: 0:38:00.\n",
      "  Batch 1,960  of  3,125.    Elapsed: 0:38:48.\n",
      "  Batch 2,000  of  3,125.    Elapsed: 0:39:35.\n",
      "  Batch 2,040  of  3,125.    Elapsed: 0:40:22.\n",
      "  Batch 2,080  of  3,125.    Elapsed: 0:41:09.\n",
      "  Batch 2,120  of  3,125.    Elapsed: 0:41:56.\n",
      "  Batch 2,160  of  3,125.    Elapsed: 0:42:43.\n",
      "  Batch 2,200  of  3,125.    Elapsed: 0:43:31.\n",
      "  Batch 2,240  of  3,125.    Elapsed: 0:44:19.\n",
      "  Batch 2,280  of  3,125.    Elapsed: 0:45:06.\n",
      "  Batch 2,320  of  3,125.    Elapsed: 0:45:54.\n",
      "  Batch 2,360  of  3,125.    Elapsed: 0:46:41.\n",
      "  Batch 2,400  of  3,125.    Elapsed: 0:47:28.\n",
      "  Batch 2,440  of  3,125.    Elapsed: 0:48:16.\n",
      "  Batch 2,480  of  3,125.    Elapsed: 0:49:03.\n",
      "  Batch 2,520  of  3,125.    Elapsed: 0:49:50.\n",
      "  Batch 2,560  of  3,125.    Elapsed: 0:50:38.\n",
      "  Batch 2,600  of  3,125.    Elapsed: 0:51:25.\n",
      "  Batch 2,640  of  3,125.    Elapsed: 0:52:13.\n",
      "  Batch 2,680  of  3,125.    Elapsed: 0:53:00.\n",
      "  Batch 2,720  of  3,125.    Elapsed: 0:53:47.\n",
      "  Batch 2,760  of  3,125.    Elapsed: 0:54:35.\n",
      "  Batch 2,800  of  3,125.    Elapsed: 0:55:22.\n",
      "  Batch 2,840  of  3,125.    Elapsed: 0:56:10.\n",
      "  Batch 2,880  of  3,125.    Elapsed: 0:56:57.\n",
      "  Batch 2,920  of  3,125.    Elapsed: 0:57:44.\n",
      "  Batch 2,960  of  3,125.    Elapsed: 0:58:32.\n",
      "  Batch 3,000  of  3,125.    Elapsed: 0:59:19.\n",
      "  Batch 3,040  of  3,125.    Elapsed: 1:00:07.\n",
      "  Batch 3,080  of  3,125.    Elapsed: 1:00:54.\n",
      "  Batch 3,120  of  3,125.    Elapsed: 1:01:42.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 1:01:48\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation took: 0:17:56\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,125.    Elapsed: 0:00:47.\n",
      "  Batch    80  of  3,125.    Elapsed: 0:01:35.\n",
      "  Batch   120  of  3,125.    Elapsed: 0:02:22.\n",
      "  Batch   160  of  3,125.    Elapsed: 0:03:09.\n",
      "  Batch   200  of  3,125.    Elapsed: 0:03:56.\n",
      "  Batch   240  of  3,125.    Elapsed: 0:04:43.\n",
      "  Batch   280  of  3,125.    Elapsed: 0:05:30.\n",
      "  Batch   320  of  3,125.    Elapsed: 0:06:18.\n",
      "  Batch   360  of  3,125.    Elapsed: 0:07:05.\n",
      "  Batch   400  of  3,125.    Elapsed: 0:07:52.\n",
      "  Batch   440  of  3,125.    Elapsed: 0:08:39.\n",
      "  Batch   480  of  3,125.    Elapsed: 0:09:26.\n",
      "  Batch   520  of  3,125.    Elapsed: 0:10:13.\n",
      "  Batch   560  of  3,125.    Elapsed: 0:11:01.\n",
      "  Batch   600  of  3,125.    Elapsed: 0:11:48.\n",
      "  Batch   640  of  3,125.    Elapsed: 0:12:37.\n",
      "  Batch   680  of  3,125.    Elapsed: 0:13:26.\n",
      "  Batch   720  of  3,125.    Elapsed: 0:14:13.\n",
      "  Batch   760  of  3,125.    Elapsed: 0:15:01.\n",
      "  Batch   800  of  3,125.    Elapsed: 0:15:49.\n",
      "  Batch   840  of  3,125.    Elapsed: 0:16:36.\n",
      "  Batch   880  of  3,125.    Elapsed: 0:17:24.\n",
      "  Batch   920  of  3,125.    Elapsed: 0:18:11.\n",
      "  Batch   960  of  3,125.    Elapsed: 0:18:59.\n",
      "  Batch 1,000  of  3,125.    Elapsed: 0:19:47.\n",
      "  Batch 1,040  of  3,125.    Elapsed: 0:20:34.\n",
      "  Batch 1,080  of  3,125.    Elapsed: 0:21:21.\n",
      "  Batch 1,120  of  3,125.    Elapsed: 0:22:08.\n",
      "  Batch 1,160  of  3,125.    Elapsed: 0:22:56.\n",
      "  Batch 1,200  of  3,125.    Elapsed: 0:23:43.\n",
      "  Batch 1,240  of  3,125.    Elapsed: 0:24:30.\n",
      "  Batch 1,280  of  3,125.    Elapsed: 0:25:17.\n",
      "  Batch 1,320  of  3,125.    Elapsed: 0:26:04.\n",
      "  Batch 1,360  of  3,125.    Elapsed: 0:26:51.\n",
      "  Batch 1,400  of  3,125.    Elapsed: 0:27:38.\n",
      "  Batch 1,440  of  3,125.    Elapsed: 0:28:26.\n",
      "  Batch 1,480  of  3,125.    Elapsed: 0:29:13.\n",
      "  Batch 1,520  of  3,125.    Elapsed: 0:30:01.\n",
      "  Batch 1,560  of  3,125.    Elapsed: 0:30:48.\n",
      "  Batch 1,600  of  3,125.    Elapsed: 0:31:35.\n",
      "  Batch 1,640  of  3,125.    Elapsed: 0:32:22.\n",
      "  Batch 1,680  of  3,125.    Elapsed: 0:33:09.\n",
      "  Batch 1,720  of  3,125.    Elapsed: 0:33:56.\n",
      "  Batch 1,760  of  3,125.    Elapsed: 0:34:44.\n",
      "  Batch 1,800  of  3,125.    Elapsed: 0:35:31.\n",
      "  Batch 1,840  of  3,125.    Elapsed: 0:36:18.\n",
      "  Batch 1,880  of  3,125.    Elapsed: 0:37:05.\n",
      "  Batch 1,920  of  3,125.    Elapsed: 0:37:52.\n",
      "  Batch 1,960  of  3,125.    Elapsed: 0:38:39.\n",
      "  Batch 2,000  of  3,125.    Elapsed: 0:39:26.\n",
      "  Batch 2,040  of  3,125.    Elapsed: 0:40:13.\n",
      "  Batch 2,080  of  3,125.    Elapsed: 0:41:01.\n",
      "  Batch 2,120  of  3,125.    Elapsed: 0:41:48.\n",
      "  Batch 2,160  of  3,125.    Elapsed: 0:42:35.\n",
      "  Batch 2,200  of  3,125.    Elapsed: 0:43:22.\n",
      "  Batch 2,240  of  3,125.    Elapsed: 0:44:09.\n",
      "  Batch 2,280  of  3,125.    Elapsed: 0:44:57.\n",
      "  Batch 2,320  of  3,125.    Elapsed: 0:45:44.\n",
      "  Batch 2,360  of  3,125.    Elapsed: 0:46:31.\n",
      "  Batch 2,400  of  3,125.    Elapsed: 0:47:18.\n",
      "  Batch 2,440  of  3,125.    Elapsed: 0:48:05.\n",
      "  Batch 2,480  of  3,125.    Elapsed: 0:48:53.\n",
      "  Batch 2,520  of  3,125.    Elapsed: 0:49:40.\n",
      "  Batch 2,560  of  3,125.    Elapsed: 0:50:27.\n",
      "  Batch 2,600  of  3,125.    Elapsed: 0:51:14.\n",
      "  Batch 2,640  of  3,125.    Elapsed: 0:52:01.\n",
      "  Batch 2,680  of  3,125.    Elapsed: 0:52:48.\n",
      "  Batch 2,720  of  3,125.    Elapsed: 0:53:36.\n",
      "  Batch 2,760  of  3,125.    Elapsed: 0:54:23.\n",
      "  Batch 2,800  of  3,125.    Elapsed: 0:55:10.\n",
      "  Batch 2,840  of  3,125.    Elapsed: 0:55:57.\n",
      "  Batch 2,880  of  3,125.    Elapsed: 0:56:44.\n",
      "  Batch 2,920  of  3,125.    Elapsed: 0:57:31.\n",
      "  Batch 2,960  of  3,125.    Elapsed: 0:58:19.\n",
      "  Batch 3,000  of  3,125.    Elapsed: 0:59:06.\n",
      "  Batch 3,040  of  3,125.    Elapsed: 0:59:53.\n",
      "  Batch 3,080  of  3,125.    Elapsed: 1:00:40.\n",
      "  Batch 3,120  of  3,125.    Elapsed: 1:01:28.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 1:01:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation took: 0:17:16\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,125.    Elapsed: 0:00:47.\n",
      "  Batch    80  of  3,125.    Elapsed: 0:01:34.\n",
      "  Batch   120  of  3,125.    Elapsed: 0:02:22.\n",
      "  Batch   160  of  3,125.    Elapsed: 0:03:09.\n",
      "  Batch   200  of  3,125.    Elapsed: 0:03:56.\n",
      "  Batch   240  of  3,125.    Elapsed: 0:04:43.\n",
      "  Batch   280  of  3,125.    Elapsed: 0:05:30.\n",
      "  Batch   320  of  3,125.    Elapsed: 0:06:17.\n",
      "  Batch   360  of  3,125.    Elapsed: 0:07:04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   400  of  3,125.    Elapsed: 0:07:51.\n",
      "  Batch   440  of  3,125.    Elapsed: 0:08:38.\n",
      "  Batch   480  of  3,125.    Elapsed: 0:09:25.\n",
      "  Batch   520  of  3,125.    Elapsed: 0:10:12.\n",
      "  Batch   560  of  3,125.    Elapsed: 0:10:59.\n",
      "  Batch   600  of  3,125.    Elapsed: 0:11:46.\n",
      "  Batch   640  of  3,125.    Elapsed: 0:12:34.\n",
      "  Batch   680  of  3,125.    Elapsed: 0:13:21.\n",
      "  Batch   720  of  3,125.    Elapsed: 0:14:08.\n",
      "  Batch   760  of  3,125.    Elapsed: 0:14:55.\n",
      "  Batch   800  of  3,125.    Elapsed: 0:15:42.\n",
      "  Batch   840  of  3,125.    Elapsed: 0:16:29.\n",
      "  Batch   880  of  3,125.    Elapsed: 0:17:17.\n",
      "  Batch   920  of  3,125.    Elapsed: 0:18:04.\n",
      "  Batch   960  of  3,125.    Elapsed: 0:18:51.\n",
      "  Batch 1,000  of  3,125.    Elapsed: 0:19:38.\n",
      "  Batch 1,040  of  3,125.    Elapsed: 0:20:25.\n",
      "  Batch 1,080  of  3,125.    Elapsed: 0:21:11.\n",
      "  Batch 1,120  of  3,125.    Elapsed: 0:21:58.\n",
      "  Batch 1,160  of  3,125.    Elapsed: 0:22:45.\n",
      "  Batch 1,200  of  3,125.    Elapsed: 0:23:32.\n",
      "  Batch 1,240  of  3,125.    Elapsed: 0:24:19.\n",
      "  Batch 1,280  of  3,125.    Elapsed: 0:25:06.\n",
      "  Batch 1,320  of  3,125.    Elapsed: 0:25:53.\n",
      "  Batch 1,360  of  3,125.    Elapsed: 0:26:40.\n",
      "  Batch 1,400  of  3,125.    Elapsed: 0:27:26.\n",
      "  Batch 1,440  of  3,125.    Elapsed: 0:28:13.\n",
      "  Batch 1,480  of  3,125.    Elapsed: 0:29:00.\n",
      "  Batch 1,520  of  3,125.    Elapsed: 0:29:47.\n",
      "  Batch 1,560  of  3,125.    Elapsed: 0:30:34.\n",
      "  Batch 1,600  of  3,125.    Elapsed: 0:31:21.\n",
      "  Batch 1,640  of  3,125.    Elapsed: 0:32:08.\n",
      "  Batch 1,680  of  3,125.    Elapsed: 0:32:55.\n",
      "  Batch 1,720  of  3,125.    Elapsed: 0:33:42.\n",
      "  Batch 1,760  of  3,125.    Elapsed: 0:34:29.\n",
      "  Batch 1,800  of  3,125.    Elapsed: 0:35:16.\n",
      "  Batch 1,840  of  3,125.    Elapsed: 0:36:02.\n",
      "  Batch 1,880  of  3,125.    Elapsed: 0:36:49.\n",
      "  Batch 1,920  of  3,125.    Elapsed: 0:37:36.\n",
      "  Batch 1,960  of  3,125.    Elapsed: 0:38:23.\n",
      "  Batch 2,000  of  3,125.    Elapsed: 0:39:10.\n",
      "  Batch 2,040  of  3,125.    Elapsed: 0:39:57.\n",
      "  Batch 2,080  of  3,125.    Elapsed: 0:40:44.\n",
      "  Batch 2,120  of  3,125.    Elapsed: 0:41:31.\n",
      "  Batch 2,160  of  3,125.    Elapsed: 0:42:18.\n",
      "  Batch 2,200  of  3,125.    Elapsed: 0:43:05.\n",
      "  Batch 2,240  of  3,125.    Elapsed: 0:43:52.\n",
      "  Batch 2,280  of  3,125.    Elapsed: 0:44:38.\n",
      "  Batch 2,320  of  3,125.    Elapsed: 0:45:25.\n",
      "  Batch 2,360  of  3,125.    Elapsed: 0:46:12.\n",
      "  Batch 2,400  of  3,125.    Elapsed: 0:46:59.\n",
      "  Batch 2,440  of  3,125.    Elapsed: 0:47:46.\n",
      "  Batch 2,480  of  3,125.    Elapsed: 0:48:33.\n",
      "  Batch 2,520  of  3,125.    Elapsed: 0:49:20.\n",
      "  Batch 2,560  of  3,125.    Elapsed: 0:50:08.\n",
      "  Batch 2,600  of  3,125.    Elapsed: 0:50:55.\n",
      "  Batch 2,640  of  3,125.    Elapsed: 0:51:42.\n",
      "  Batch 2,680  of  3,125.    Elapsed: 0:52:29.\n",
      "  Batch 2,720  of  3,125.    Elapsed: 0:53:16.\n",
      "  Batch 2,760  of  3,125.    Elapsed: 0:54:03.\n",
      "  Batch 2,800  of  3,125.    Elapsed: 0:54:50.\n",
      "  Batch 2,840  of  3,125.    Elapsed: 0:55:37.\n",
      "  Batch 2,880  of  3,125.    Elapsed: 0:56:24.\n",
      "  Batch 2,920  of  3,125.    Elapsed: 0:57:11.\n",
      "  Batch 2,960  of  3,125.    Elapsed: 0:57:58.\n",
      "  Batch 3,000  of  3,125.    Elapsed: 0:58:45.\n",
      "  Batch 3,040  of  3,125.    Elapsed: 0:59:33.\n",
      "  Batch 3,080  of  3,125.    Elapsed: 1:00:20.\n",
      "  Batch 3,120  of  3,125.    Elapsed: 1:01:07.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 1:01:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation took: 0:17:17\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  3,125.    Elapsed: 0:00:47.\n",
      "  Batch    80  of  3,125.    Elapsed: 0:01:34.\n",
      "  Batch   120  of  3,125.    Elapsed: 0:02:22.\n",
      "  Batch   160  of  3,125.    Elapsed: 0:03:09.\n",
      "  Batch   200  of  3,125.    Elapsed: 0:03:56.\n",
      "  Batch   240  of  3,125.    Elapsed: 0:04:43.\n",
      "  Batch   280  of  3,125.    Elapsed: 0:05:30.\n",
      "  Batch   320  of  3,125.    Elapsed: 0:06:17.\n",
      "  Batch   360  of  3,125.    Elapsed: 0:07:05.\n",
      "  Batch   400  of  3,125.    Elapsed: 0:07:52.\n",
      "  Batch   440  of  3,125.    Elapsed: 0:08:39.\n",
      "  Batch   480  of  3,125.    Elapsed: 0:09:26.\n",
      "  Batch   520  of  3,125.    Elapsed: 0:10:13.\n",
      "  Batch   560  of  3,125.    Elapsed: 0:11:00.\n",
      "  Batch   600  of  3,125.    Elapsed: 0:11:48.\n",
      "  Batch   640  of  3,125.    Elapsed: 0:12:34.\n",
      "  Batch   680  of  3,125.    Elapsed: 0:13:22.\n",
      "  Batch   720  of  3,125.    Elapsed: 0:14:09.\n",
      "  Batch   760  of  3,125.    Elapsed: 0:14:56.\n",
      "  Batch   800  of  3,125.    Elapsed: 0:15:43.\n",
      "  Batch   840  of  3,125.    Elapsed: 0:16:30.\n",
      "  Batch   880  of  3,125.    Elapsed: 0:17:17.\n",
      "  Batch   920  of  3,125.    Elapsed: 0:18:05.\n",
      "  Batch   960  of  3,125.    Elapsed: 0:18:52.\n",
      "  Batch 1,000  of  3,125.    Elapsed: 0:19:39.\n",
      "  Batch 1,040  of  3,125.    Elapsed: 0:20:26.\n",
      "  Batch 1,080  of  3,125.    Elapsed: 0:21:13.\n",
      "  Batch 1,120  of  3,125.    Elapsed: 0:22:00.\n",
      "  Batch 1,160  of  3,125.    Elapsed: 0:22:47.\n",
      "  Batch 1,200  of  3,125.    Elapsed: 0:23:35.\n",
      "  Batch 1,240  of  3,125.    Elapsed: 0:24:21.\n",
      "  Batch 1,280  of  3,125.    Elapsed: 0:25:09.\n",
      "  Batch 1,320  of  3,125.    Elapsed: 0:25:56.\n",
      "  Batch 1,360  of  3,125.    Elapsed: 0:26:43.\n",
      "  Batch 1,400  of  3,125.    Elapsed: 0:27:30.\n",
      "  Batch 1,440  of  3,125.    Elapsed: 0:28:17.\n",
      "  Batch 1,480  of  3,125.    Elapsed: 0:29:04.\n",
      "  Batch 1,520  of  3,125.    Elapsed: 0:29:51.\n",
      "  Batch 1,560  of  3,125.    Elapsed: 0:30:38.\n",
      "  Batch 1,600  of  3,125.    Elapsed: 0:31:25.\n",
      "  Batch 1,640  of  3,125.    Elapsed: 0:32:12.\n",
      "  Batch 1,680  of  3,125.    Elapsed: 0:33:00.\n",
      "  Batch 1,720  of  3,125.    Elapsed: 0:33:47.\n",
      "  Batch 1,760  of  3,125.    Elapsed: 0:34:34.\n",
      "  Batch 1,800  of  3,125.    Elapsed: 0:35:21.\n",
      "  Batch 1,840  of  3,125.    Elapsed: 0:36:08.\n",
      "  Batch 1,880  of  3,125.    Elapsed: 0:36:55.\n",
      "  Batch 1,920  of  3,125.    Elapsed: 0:37:42.\n",
      "  Batch 1,960  of  3,125.    Elapsed: 0:38:29.\n",
      "  Batch 2,000  of  3,125.    Elapsed: 0:39:16.\n",
      "  Batch 2,040  of  3,125.    Elapsed: 0:40:04.\n",
      "  Batch 2,080  of  3,125.    Elapsed: 0:40:51.\n",
      "  Batch 2,120  of  3,125.    Elapsed: 0:41:38.\n",
      "  Batch 2,160  of  3,125.    Elapsed: 0:42:25.\n",
      "  Batch 2,200  of  3,125.    Elapsed: 0:43:12.\n",
      "  Batch 2,240  of  3,125.    Elapsed: 0:43:59.\n",
      "  Batch 2,280  of  3,125.    Elapsed: 0:44:46.\n",
      "  Batch 2,320  of  3,125.    Elapsed: 0:45:33.\n",
      "  Batch 2,360  of  3,125.    Elapsed: 0:46:20.\n",
      "  Batch 2,400  of  3,125.    Elapsed: 0:47:07.\n",
      "  Batch 2,440  of  3,125.    Elapsed: 0:47:54.\n",
      "  Batch 2,480  of  3,125.    Elapsed: 0:48:41.\n",
      "  Batch 2,520  of  3,125.    Elapsed: 0:49:28.\n",
      "  Batch 2,560  of  3,125.    Elapsed: 0:50:15.\n",
      "  Batch 2,600  of  3,125.    Elapsed: 0:51:02.\n",
      "  Batch 2,640  of  3,125.    Elapsed: 0:51:49.\n",
      "  Batch 2,680  of  3,125.    Elapsed: 0:52:36.\n",
      "  Batch 2,720  of  3,125.    Elapsed: 0:53:23.\n",
      "  Batch 2,760  of  3,125.    Elapsed: 0:54:11.\n",
      "  Batch 2,800  of  3,125.    Elapsed: 0:54:58.\n",
      "  Batch 2,840  of  3,125.    Elapsed: 0:55:44.\n",
      "  Batch 2,880  of  3,125.    Elapsed: 0:56:31.\n",
      "  Batch 2,920  of  3,125.    Elapsed: 0:57:18.\n",
      "  Batch 2,960  of  3,125.    Elapsed: 0:58:04.\n",
      "  Batch 3,000  of  3,125.    Elapsed: 0:58:51.\n",
      "  Batch 3,040  of  3,125.    Elapsed: 0:59:37.\n",
      "  Batch 3,080  of  3,125.    Elapsed: 1:00:24.\n",
      "  Batch 3,120  of  3,125.    Elapsed: 1:01:11.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 1:01:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation took: 0:17:15\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Seed value for reproducibility\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# we set our device to CPU.\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS (GPU) is available. Using GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU instead.\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Move the model to the CPU\n",
    "model.to(device)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['label'].to(device)\n",
    "\n",
    "        # clear any previously calculated gradients before performing a\n",
    "        # backward pass. \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure performance on\n",
    "    # validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['label'].to(device)\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68713b1",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b087fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (20.9)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (52.0.0.post20210125)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.22.4)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow --timeout=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbd4195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "391/391 [==============================] - 1348s 3s/step - loss: 0.4966 - accuracy: 0.7270 - val_loss: 0.2798 - val_accuracy: 0.8876\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 1148s 3s/step - loss: 0.2331 - accuracy: 0.9084 - val_loss: 0.2762 - val_accuracy: 0.8872\n",
      "391/391 [==============================] - 362s 926ms/step - loss: 0.2762 - accuracy: 0.8872\n",
      "Test Loss: 0.2762269377708435\n",
      "Test Accuracy: 0.8871999979019165\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the IMDB dataset\n",
    "(train_data, test_data), info = tfds.load('imdb_reviews', split=['train', 'test'], with_info=True, as_supervised=True)\n",
    "\n",
    "# Preprocess the data\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def preprocess_text(text, label):\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"<br />\", \" \")\n",
    "    text = tf.strings.regex_replace(text, \"[^a-zA-Z']\", \" \")\n",
    "    return text, label\n",
    "\n",
    "train_data = train_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).map(preprocess_text)\n",
    "test_data = test_data.batch(BATCH_SIZE).map(preprocess_text)\n",
    "\n",
    "# Text Vectorization\n",
    "VOCAB_SIZE = 10000\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "# Adapt the encoder on the train data\n",
    "encoder.adapt(train_data.map(lambda text, label: text))\n",
    "\n",
    "# Transformer Model\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    # Attention and Normalization\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attention_output = tf.keras.layers.Dropout(rate)(attention_output)\n",
    "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "\n",
    "    # Feed Forward Network\n",
    "    ff_output = tf.keras.layers.Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ff_output = tf.keras.layers.Dense(embed_dim)(ff_output)\n",
    "    ff_output = tf.keras.layers.Dropout(rate)(ff_output)\n",
    "    ff_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + ff_output)\n",
    "    return ff_output\n",
    "\n",
    "# Model Creation\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "x = encoder(inputs)\n",
    "x = tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=embed_dim)(x)\n",
    "x = transformer_encoder(x, embed_dim, num_heads, ff_dim)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, epochs=2, validation_data=test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
